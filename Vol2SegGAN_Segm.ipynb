{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_Vol2SegGAN_Segm.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OQyQC-thfJWu","executionInfo":{"status":"ok","timestamp":1616613621530,"user_tz":-180,"elapsed":57421,"user":{"displayName":"Gaffari Ã‡elik","photoUrl":"","userId":"05436464457074694596"}},"outputId":"6adcd71e-800c-42d9-d2f1-08515d33312c"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","%cd 'drive/My Drive'\n","%cd 'Vol2SegGAN' ## project path \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive\n","/content/drive/My Drive/IBSR/3DUnet_GAN_MIPS_Patch/15_Github_Vol2SegGAN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-HoCwEjpwgMy"},"source":["\n","!pip install tensorflow-gpu==2.1\n","!pip install tensorflow-addons==0.9.1\n","!pip install time\n","!pip install SimpleITK\n","!pip install natsort"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4tmco0tizvgq"},"source":["\n","import tensorflow.compat.v1 as tf\n","from random import shuffle\n","from natsort import natsorted, ns\n","from data_load import DataGenerator\n","import glob\n","import os\n","import config\n","FLAGS = tf.app.flags.FLAGS\n","\n","\n","def data(data_selected):\n","\n","    path = FLAGS.data_dir+data_selected+\"/\"\n","        \n","    data_list = natsorted(os.listdir(path), alg=ns.PATH | ns.IGNORECASE)\n","    shuffle(data_list)\n","    \n","    path=path+'*/'\n","    t1_list = sorted(glob.glob(path+'*t1_strip_registration.nii.gz'))\n","    seg_list = sorted(glob.glob(path+'*segm_registration_round_class.nii.gz'))#sorted(glob.glob(path+'*segm.nii.gz'))\n","    \n","    print(t1_list)\n","    veri=[]\n","    for i in data_list:\n","        i=int(i)-1\n","        veri.append([t1_list[i], seg_list[i]])\n","    return veri\n","\n","sets = {'train': [], 'valid': [], 'test': []}\n","sets['train']=data(data_selected='train')\n","train_gen = DataGenerator(sets['train'],batch_size=FLAGS.batch_size)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OsaY5Ojxzgus"},"source":["from tensorflow.keras.models import Model,load_model,model_from_json\n","from tensorflow.keras.layers import Input, Activation\n","from tensorflow.keras.optimizers import Adam\n","#import tensorflow.keras.backend as K\n","import os\n","import numpy as np\n","from utils2 import  get_dsc\n","import tensorflow.compat.v1 as tf\n","import time\n","from model import Vol2SegGAN\n","from random import randint\n","\n","class_weights = np.load('class_weights.npy')\n","\n","class GAN_train():\n","    def __init__(self, img_shape, seg_shape, class_weights, Nfilter_start=32, depth=4, batch_size=1, LAMBDA=5):\n","        self.img_shape = img_shape\n","        self.seg_shape = seg_shape\n","        self.class_weights = class_weights\n","        self.Nfilter_start = Nfilter_start\n","        self.depth = depth\n","        self.batch_size = batch_size\n","        self.LAMBDA = LAMBDA\n","        self.path = FLAGS.save_model\n","\n","        def diceLoss(y_true, y_pred, w=self.class_weights):\n","            y_true = tf.convert_to_tensor(y_true, 'float32')\n","            y_pred = tf.convert_to_tensor(y_pred, y_true.dtype)\n","            num = tf.math.reduce_sum(tf.math.multiply(w, tf.math.reduce_sum(tf.math.multiply(y_true, y_pred), axis=[0,1,2,3])))\n","            den = tf.math.reduce_sum(tf.math.multiply(w, tf.math.reduce_sum(tf.math.add(y_true, y_pred), axis=[0,1,2,3])))+1e-5\n","            return 1-2*num/den\n","\n","         \n","        if os.path.exists(self.path)==False:\n","            os.mkdir(self.path)               \n","\n","        if (os.path.exists(self.path+'generator.h5')):          \n","\n","          import json\n","          #model_json= self.generator.to_json()\n","          with open(self.path+'generator.json', \"r\") as f:\n","            model_json = json.load(f)\n","\n","          self.generator=model_from_json(model_json)\n","          self.generator.load_weights(self.path+'generator.h5')\n","                \n","          with open(self.path+'discriminator.json', \"r\") as f:\n","            model_discriminator = json.load(f)\n","            \n","          self.discriminator=model_from_json(model_discriminator)\n","          self.discriminator.load_weights(self.path+'discriminator.h5')\n","          self.discriminator.compile(loss='mse', optimizer=Adam(1e-4, beta_1=0.5), metrics=['accuracy'])\n","          \n","          with open(self.path+'Vol2SegGAN.json', \"r\") as f:\n","            model_combined = json.load(f)\n","\n","          self.combined=model_from_json(model_combined)\n","          self.combined.load_weights(self.path+'Vol2SegGAN.h5')\n","          self.combined.compile(loss=['mse', diceLoss], optimizer=Adam(1e-4, beta_1=0.5))\n","          print('loaded')         \n","\n","        else:\n","          # Build and compile the discriminator\n","          gan = Vol2SegGAN(self.img_shape, self.seg_shape, Nfilter_start=self.Nfilter_start,depth=self.depth)\n","          self.discriminator = gan.Discriminator()\n","          self.discriminator.compile(loss='mse', optimizer=Adam(1e-4, beta_1=0.5), metrics=['accuracy'])\n","          self.discriminator.summary()\n","\n","          # Build the generator\n","          self.generator = gan.Generator()\n","          self.generator.summary()\n","          # Input images and their conditioning images\n","          seg = Input(shape=self.seg_shape)\n","          img = Input(shape=self.img_shape)\n","\n","          # By conditioning on B generate a fake version of A\n","          seg_pred = self.generator(img)\n","\n","          # For the combined model we will only train the generator\n","          self.discriminator.trainable = False\n","\n","          # Discriminators determines validity of translated images / condition pairs\n","          # print(seg_pred.shape)\n","          valid = self.discriminator([seg_pred, img])\n","\n","          self.combined = Model(inputs=[seg, img], outputs=[valid, seg_pred])\n","          self.combined.compile(loss=['mse', diceLoss], loss_weights=[1, self.LAMBDA], optimizer=Adam(1e-4, beta_1=0.5))\n","          print('New Model')\n","\n","    def train_step(self, Xbatch, Ybatch):\n","        # Generetor output\n","        gen_output = self.generator.predict(Xbatch, use_multiprocessing=True, workers=16)\n","        \n","        # Discriminator output shape    \n","        disc_output_shape = self.discriminator.output_shape\n","        disc_output_shape = (gen_output.shape[0], *disc_output_shape[1:])\n","        \n","        # Train Discriminator\n","        disc_loss_real = self.discriminator.fit([Ybatch, Xbatch], tf.ones(disc_output_shape), verbose=0, use_multiprocessing=True, workers=16)\n","        disc_loss_fake = self.discriminator.fit([gen_output, Xbatch], tf.zeros(disc_output_shape), verbose=0, use_multiprocessing=True, workers=16)\n","\n","        # Train Generator\n","        gen_loss = self.combined.fit([Ybatch, Xbatch], [tf.ones(disc_output_shape), Ybatch], verbose=0, use_multiprocessing=True, workers=16)        \n","        dsc = get_dsc(labels=Ybatch,predictions=gen_output)        \n","        return gen_loss,dsc\n","    \n","\n","    def save_model(self,path):\n","        import json\n","\n","        model_generator= self.generator.to_json()\n","        with open((path+\"generator.json\"), \"w\") as json_file:\n","            json.dump(model_generator, json_file)\n","        self.generator.save_weights(path+\"generator.h5\")\n","        #self.generator.save((path+\"/generator.model\")) \n","        \n","        model_discriminator= self.discriminator.to_json()\n","        with open((path+\"discriminator.json\"), \"w\") as json_file:\n","            json.dump(model_discriminator, json_file)\n","        self.discriminator.save_weights(path+\"discriminator.h5\")\n","        #self.discriminator.save((path+\"/discriminator.model\"))  \n","\n","        model_combined= self.combined.to_json()\n","        with open((path+\"Vol2SegGAN.json\"), \"w\") as json_file:\n","            json.dump(model_combined, json_file)\n","        self.combined.save_weights(path+\"Vol2SegGAN.h5\")        \n","        #self.combined.save((path+\"/Vol2SegGAN.model\")) \n","\n","    \n","    def tic(self):\n","        global _start_time\n","        _start_time = time.time()\n","\n","    def toc(self):\n","        t_sec = round(time.time() - _start_time)\n","        (t_min, t_sec) = divmod(t_sec,60)\n","        (t_hour,t_min) = divmod(t_min,60)\n","        sure='Time: {}saat:{}dk:{}sn'.format(t_hour,t_min,t_sec)\n","        print(sure)   \n","\n","    def run_toc(self,start_time):\n","      t_sec = round(time.time() - start_time)\n","      (t_min1, t_sec) = divmod(t_sec,60)\n","      (t_hour,t_min) = divmod(t_min1,60)\n","      return t_min1\n","\n","    def train(self):\n","        print('Training process:')\n","        PERIOD_OF_TIME = time.time()\n","        start_time=time.time() \n","     \n","        max_steps=FLAGS.max_steps\n","        self.tic()\n","        for itr in range(1,FLAGS.max_steps): \n","          train_gen = DataGenerator(sets['train'], batch_size=self.batch_size)     \n","          for Xbatch, Ybatch in train_gen:\n","              gan_losses,s_dsc = self.train_step(Xbatch, Ybatch)\n","              train_status = '- step: {}/{} : loss: {:0.4f} - bgr:{:0.3f} csf:{:0.3f} gm:{:0.3f} wm:{:0.3f} '\n","              txt=(train_status.format(itr, FLAGS.max_steps,gan_losses.history['loss'][0], s_dsc[0], s_dsc[1],s_dsc[2],s_dsc[3]))      \n","              print(txt)\n","          gecen_sure=self.run_toc(PERIOD_OF_TIME)\n","          if (itr)%FLAGS.steps_to_save_checkpoint == 0: \n","            ## Save_model\n","            self.save_model(path=self.path)\n","            print(\"Save checkpoint\")     \n","            print(itr)\n","        self.toc()       \n","        return trends_train, trends_valid\n","patch_size = list(map(int, FLAGS.patch_size.split(\",\")))\n","px =patch_size[0]\n","py =patch_size[1]\n","pz =patch_size[2] \n","\n","  \n","imShape = (px, py, pz, 1) \n","gtShape = (px, py, pz, 4)\n","\n","gan = GAN_train(imShape, gtShape, class_weights, Nfilter_start=FLAGS.Nfilter_start,depth=FLAGS.depth, batch_size=FLAGS.batch_size, LAMBDA=FLAGS.LAMBDA)\n","\n","trends_train, trends_valid = gan.train()\n"],"execution_count":null,"outputs":[]}]}